---
title: (ChatGPT) ChatGPT의 학습 방법과 기본원리
author: Jake
date: 2023-08-28 13:00:00 +0900
categories: [ChatGPT, GPT_Basic]
tags: [AI, ChatGPT, GPT, OpenAI]
pin: true
math: true
mermaid: true
image:
#  path: jetbrains://idea/navigate/reference?project=Uk-jake.github.io&path=_posts/assets/0828/ChatGPT.png
#  lqip: data:image/webp;base64,UklGRpoAAABXRUJQVlA4WAoAAAAQAAAADwAABwAAQUxQSDIAAAARL0AmbZurmr57yyIiqE8oiG0bejIYEQTgqiDA9vqnsUSI6H+oAERp2HZ65qP/VIAWAFZQOCBCAAAA8AEAnQEqEAAIAAVAfCWkAALp8sF8rgRgAP7o9FDvMCkMde9PK7euH5M1m6VWoDXf2FkP3BqV0ZYbO6NA/VFIAAAA
#  alt: Responsive rendering of Chirpy theme on multiple devices.
---
## 글을 작성하게 된 이유

<aside>
💡 ChatGPT의 출시 이후, GPT에게 도움을 받는 일이 증가하며 업무 스타일이 변화하고 있습니다. 초기에는 문법 검사, 제목 추천, 정보 검색과 같이 간단한 작업에 GPT의 도움을 받았지만, 요즘에는 코드 리뷰, 목차 작성과 같이 복잡한 일도 GPT의 도움을 받고 있습니다. 이러한 사용은 일의 효율성은 크게 향상시켰지만, 동시에 GPT를 활용하면서 늘어난 걱정도 존재합니다. GPT를 이용하면서 남의 지식을 활용하는 듯한 느낌과 나만의 역량이 저하되는 느낌이 들기 때문입니다. 이러한 걱정을 해소하기 위해 GPT의 기본 개념과 GPT한계점등을 정리하고자 합니다.

</aside>

---

# ChatGPT의 개념

## ChatGPT란?

![ChatGPT](https://github.com/Uk-jake/WebDev2022_/assets/100981076/2939a6b1-e0f2-4232-852b-c8b21bfd548d)

ChatGPT란 OpenAI에서 개발한 GPT-3.5 기반의 **대형 언어 모델(Large Language Model, LLM)** 챗봇입니다. 대형 언어모델(LLM)은 일련의 단어에서 다음 단어를 예측하는 작업을 수행하는 생성형 모델입니다. 

또한, ChatGPT는 **인간 피드백형 강화학습 (Reinforcement Learning with Human Feedback, RLHF)** 을 사용하는데, 이는 사용자의 질문에 만족스러운 반응을 생성하는 능력을 학습하기 위해 인간 피드백을 사용하는 추가 훈련 계층을 말합니다.

## ChatGPT의 기본 원리

ChatGPT의 원리는 크게 **LLM(Large Language Model, 대형 언어 모델)**과 **RLHF(Reinforcement Learning with Human Feedback, 인간 피드백형 강화 학습)**으로 나뉩니다.

LLM은 거대 언어 모델로 문장에서 다음에 오는 단어를 정확하게 예측하는 모델입니다. 

언어 모델이란 주어진 이전 단어들을 바탕으로 다음에 나올 단어나 문장을 예측하는 모델입니다. 

만약 “나는 저녁을 ___ ” 이라는 문장이 있다면 뒤에 올 수 있는 단어를 

- 먹었다.
- 먹을 것이다.

이런 식으로 예측하는 것입니다. 

언어 모델은 가지고 있는 데이터들을 가지고 학습하면서 다음에 올 문장들을 예측하는 것입니다. 그러므로 데이터의 양은 많을 수록 좋은 것입니다. 당연히 GPT는 많은 매개변수와 데이터를 기반으로 좋은 성능을 낼 수 있다고 합니다. 스탠포드 대학에 따르면 **GPT-3는 1,750억 개의 매개변수**를 가지고 있으며 **570기가바이트의 텍스트**에 대해 교육을 받았다고 합니다. 이러한 LLM은 많은 데이터를 사용하여 다음 문장을 예측하는 수행능력은 증가하였지만 사용자가 원하는 것은 정확히 이해하지 못한다는 한계점을 가지고 있습니다.이러한 한계점은 RLHF 훈련을 통해 해결하였습니다. 이 훈련을 통해 ChatGPT는 사용자의 지시를 따르고 만족스러운 반응을 생성하는 능력을 만들 수 있게 된 것입니다.

## ChatGPT의 학습 원리

LLM의 한계점을 극복하기 위해서 ChatGPT는 RLHF 강화학습 방법을 사용하여 훈련시켰습니다.

RLHF는 크게 3단계로 이루어져 있습니다.

![RLHF](https://github.com/Uk-jake/WebDev2022_/assets/100981076/a531e04c-ab93-4ae8-af2b-ef4a0d3b15c6)

### 1단계: 사람의 피드백을 기반으로 인공지능 모델을 학습합니다.

**이미 공개된 API를 통해 수집된 사람들의 지시(instruction)에 대해서 인공지능이 응답을 어떻게 해야하는지 사람이 직접 정답을 작성합니다. [일반적인 목적형 대화 (Task-oriented Dialogue) 데이터셋](https://github.com/KLUE-benchmark/KLUE/tree/main/klue_benchmark/wos-v1.1) 을 제작하는 것과 유사한 방식을 따랐는데, 인공지능과 사람이 대화하는 상황을 설정하고 사람이 인공지능과 인공지능 사용자의 역할 양 쪽을 모두 연기하며 대화를 진행하고 이를 대화 데이터로 축적합니다. GPT-3.5를 이 데이터셋을 바탕으로 추가 학습을 진행하여 ChatGPT의 초기 버전을 제작합니다.**

### 2단계: 사람이 선호하는 응답을 평가하는 인공지능 모델을 학습합니다.

**1단계에서 학습한 ChatGPT의 초기 버전이 동일한 지시(instruction)에 대해서 가능한 여러 개의 응답을 생성하고, [사람이 보기에 무엇이 더 나은지 순위를 매긴 데이터셋](https://github.com/openai/following-instructions-human-feedback/tree/main/automatic-eval-samples)을 구축합니다. 이 과정에 필요한 이유는 사람의 응답 평가 과정을 자동화하기 위해 평가용 인공지능(Reward Model, 보상 모델)을 학습하기 위함입니다. 평가용 인공지능은 주어진 지시(instruction)에 가능한 응답 중에서 어떤 것이 사람이 보기에 더 나은지를 학습하여 사람의 인공지능 평가 과정을 수월하게 만들어 줍니다.**

### 3단계: 강화학습 기법을 활용하여 ChatGPT를 학습합니다.

**1단계에서 학습한 대화형 인공지능과 2단계에서 학습한 평가용 인공지능을 활용하여 ChatGPT 초기 버전의 성능을 끌어올립니다. ChatGPT의 초기 버전은 다양한 사람의 지시에 맞추어 응답을 생성하고, 생성한 응답이 얼마나 좋은지 평가용 인공지능(Reward Model)이 평가하여, ChatGPT가 점차 더 좋은 답변을 생성할 수 있도록 유도합니다. [학습 알고리즘은 PPO 강화 학습](https://github.com/allenai/RL4LMs)기법을 활용합니다. [OpenAI는 이 과정을 여러 번 반복](https://openai.com/blog/our-approach-to-alignment-research/)하여 현재의 ChatGPT를 만들었다고 합니다.**

## **ChatGPT의 제한 사항**

1. **유독성 반응에 대한 한계**
    
    ChatGPT는 사회적으로 부적절하거나 유해한 내용을 생성하지 않도록 프로그래밍되었습니다. 따라서 ChatGPT는 유독성, 혐오성, 차별성 등을 포함한 그러한 종류의 질문이나 요청에 대해 답변을 제공하지 않습니다.
    
2. **방향의 품질에 따른 답변의 품질**
    
    ChatGPT의 한계 중 하나는 출력 답변의 품질이 입력된 방향(프롬프트)의 품질에 크게 의존한다는 점입니다. 사용자가 제공한 프롬프트가 충분히 명확하고 구체적이면 더 나은 답변을 생성할 가능성이 높아집니다. 반면, 모호한 프롬프트는 모델이 부정확한 또는 원하지 않는 결과를 생성할 수 있게 할 수 있습니다.
    
3. **부정확한 답변 제공**
    
    또 다른 제한 사항은 ChatGPT가 훈련 데이터 내에서 학습한 내용을 기반으로 답변을 생성한다는 점입니다. 이로 인해 가끔 모델이 사용자가 옳다고 생각하는 내용과 다른 답변을 생성할 수 있습니다. 사용자는 생성된 답변을 신중하게 검토하고 판단하여 정보의 정확성을 확보해야 합니다.
    

ChatGPT의 제한 사항을 이해하는 것은 모델을 적절하게 활용하고 그 결과를 신중하게 고려하는데 도움이 됩니다. 이러한 제한 사항을 고려하여 모델을 사용하면 보다 유익하고 안전한 경험을 얻을 수 있습니다.

## 정리 및 의견

ChatGPT를 효과적으로 활용하기 위해 출시 배경, 기본 원리, 그리고 제한 사항을 간략히 알아보았습니다. 이를 통해 AI 기술이 얼마나 발전했는지 인지할 수 있는 기회였습니다. 저의 생각으로는 아직 많은 분들이 AI의 등장으로 인해 일자리가 줄어들 것이라고 걱정하고 계실 것으로 생각합니다. 그러나 이러한 기술의 발전은 더 많은 일자리 창출과 함께, 저희의 역량을 한 단계 더 높여줄 수 있는 기회를 제공할 것이라 믿습니다. 그렇기에 우리는 어떻게 AI를 우리의 분야에 접목시켜 나아갈 수 있는지를 고민하는 것도 중요한 부분이라고 생각합니다.

한편으로, ChatGPT의 정확도와 신뢰성은 아직 완벽하지 않은 것 같습니다. 제한 사항에서 언급된 사실이 아닌 정보를 그럴듯하게 생성하는 문제는 여전히 존재합니다. 따라서 사용자들은 생성된 답변이 실제로 정확한지를 확인하기 위해 추가적인 검토 과정을 거쳐야 할 것으로 생각됩니다.

(블로그 내용 중 틀린 부분이 있으면 댓글로 알려주시면 감사하겠습니다! 추가적인 피드백도 언제나 환영입니다. 긴 글 읽어주셔서 감사합니다.)

### 참고

[ChatGPT 원리는 무엇이고, 어떻게 학습되었을까?](https://devocean.sk.com/blog/techBoardDetail.do?ID=164626&boardType=techBlog)

[ChatGPT(챗GPT)란 무엇인가? - 원리, 예시, 활용법 - TBWA 데이터랩](https://seo.tbwakorea.com/blog/what-is-chatgpt/)

[ChatGPT의 학습 원리와 활용법](https://blog.softly.ai/how-chatgpt-learns-world/)

[ChatGPT의 원리가 궁금하시다면 들어오세요😉](https://m.blog.naver.com/1strider/223015532860)
